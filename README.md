# Neural Network From Scratch in Python
This project aims to implement a neural network library from scratch using Python, based on the concepts and techniques outlined in the book "Neural Network from Scratch in Python" by Harrison Kinsley. 

## Features
- Feedforward algorithm: The library implements the feedforward algorithm, enabling the network to process inputs, compute activations, and generate outputs.
- Backpropagation algorithm: Backpropagation is implemented to calculate gradients, update weights and biases, and modify the network's performance during the training process.
- Activation functions: The library supports a range of activation functions, including Sigmoid, ReLU, and Softmax, allowing users to experiment with different non-linearities.
- Gradient descent optimization: The library incorporates gradient descent optimization algorithms, such as Stochastic Gradient Descent (SGD), to efficiently minimize the network's loss function.

## Pre-requisite
In order to get a better understanding of the project, you need to have prior knowledge in the following subjects:
- Mathematics: Linear Algebra, Calculus III (Multivariable Calculus)
- Programming: NumPy and Python

## Acknowledgments
The implementation of this neural network library draws from the book "Neural Network from Scratch in Python" by Harrison Kinsley. Special thanks to the author for providing valuable insights in understanding and implementing the neural network library, and to my friend Eric who had introduced me this amazing book.
